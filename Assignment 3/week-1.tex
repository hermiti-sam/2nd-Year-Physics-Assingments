\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{kpfonts}
\setlength{\textwidth}{7.5in}
 \setlength{\evensidemargin}{-0.5in}
 \setlength{\oddsidemargin}{-0.5in}
 \setlength{\topmargin}{-1.2in}
 \setlength{\textheight}{10.2in}
 \setlength{\textfloatsep}{10pt}
\pagenumbering{gobble}
\usepackage{graphicx}
\usepackage{url}
%\usepackage{CJKutf8}
%\usepackage{xeCJK}
%\setCJKmainfont{SimSun}

%\noindent {\bf \Large MATH34600 \hfill Information Theory \hfill 2019-20}\\


\date{}
\begin{document}
\vspace{-0.95in}
\begin{center}
{\Large{MATH34600, 2021-`22 \hfill Information Theory \hfill Week-1: Summary}}\\
\end{center}

%\vspace{0.05in}
{\bf Proofs/examples/intuition/figures generally omitted from these notes --- please see handwritten notes/videos.}
%\vspace{-0.11in}
%\small{Group Name: \hspace{2in} Name: \hspace{2in} Student ID:\\
%\vspace{0.05in}


\begin{enumerate}
\item {\bf \underline{Big-Oh notation}}:
\begin{enumerate}
	\item $f(n) \in {\cal O}(g(n))$ if $\exists$ $c > 0$, $N \in {\mathbb Z}$ such that $\forall n > N$, $|f(n)| < c|g(n)|$.
	\item $f(n) \in o(g(n))$ if  $\forall c > 0$, $\exists N_c \in {\mathbb Z}$ such that $\forall n > N_c$, $|f(n)| < c|g(n)|$.
	\item $f(n) \in \Omega(g(n))$ if $g(n) \in {\cal O}(f(n))$
	\item $f(n) \in \omega(g(n))$ if $g(n) \in o(f(n))$
	\item $f(n) \in \Theta(g(n))$ if $f(n) \in {\cal O}(g(n))$ and $f(n) \in \Omega(g(n))$
	\item $f(n) \doteq g(n)$ if $\lim_{n\rightarrow \infty} \frac{\log(f(n)}{\log(g(n))} = 1 $, $f(n) \dot{\leq} g(n)$ if $\lim_{n\rightarrow \infty} \frac{\log(f(n)}{\log(g(n))} \leq 1 $, and $f(n) \dot{\geq} g(n)$ if $\lim_{n\rightarrow \infty} \frac{\log(f(n)}{\log(g(n))} \geq 1 $
\end{enumerate}

\item {\bf \underline{Stirling's approximation}}: $n! \doteq \sqrt{2\pi n}\left (\frac{n}{e} \right )^n\left ( 1 + \Theta\left ( \frac{1}{n}\right) \right)$.


\item {\bf \underline{Notational conventions}}: 
\begin{enumerate}
	\item All logarithms $\log(\cdot)$ and exponentials $\exp(\cdot)$ will be base-$2$ (unless explicitly specified others), since a ``natural'' unit of information is bits.
	\item Sets will be denoted with calligraphic letters, e.g. $\cal{X}$.
	\item \label{(c)} Vectors will be denoted with underlines, e.g. $\underline{x}$.
	\item \label{(d)} Random variables will be denoted by capital letters, e.g. $X$. So (combined with the previous convention) random {\it vectors} will be underlined capital letters, e.g. $\underline{X}$.
	\item Notation such as $p_X(\cdot)$ references the probability mass function (p.m.f.) of random variable $X$. So $p_X(i)$ equals the probability that the random variable $X$ takes the value $i$.
		\item For any vector $\underline{v} \in {\mathbb R}^n$, the $\ell_1$-norm $||\underline{v} ||_1 \triangleq \sum_{i=1}^n |v_i|$.
		\item We denote the length of a vector $\underline{v}$ as $len(\underline{v})$.
\end{enumerate}

\item {\bf \underline{Probability of meeting expectations for binary random variables}}: Let $X_i$ be i.i.d. binary random variables, $X_i \sim p_X$, where $p_X = (1-p,p)$. Let $\bar{X} \triangleq \sum_{i=1}^n X_i$. Then ${\mathbb E}(\bar{X}) = pn$, and  $\Pr(\bar{X} = {\mathbb E}(\bar{X})) = {{n}\choose{pn}}p^{pn}(1-p)^{(1-p)n} = \frac{1}{\sqrt{2\pi p(1-p)n}} \left (1 + \Theta\left ( \frac{1}{n}\right ) \right ) $

\item {\bf \underline{Types/Type-classes}}: Let ${\cal X} = \{1,2,\ldots,|{\cal X}|\}$. 
	\begin{enumerate}
		\item The {\it type} $T(\underline{x})$ of a vector $\underline{x} \in {\cal X}^n$ length-$|\cal X|$ probability vector whose $i$th component equals the fraction of occurrences of the symbol $i$ in $\underline{x}$. 
		\item The {\it type-class} ${\cal T}(\underline{x})$ of the vector $\underline{x}$ is the set of all vectors $\underline{x}' \in {\cal X}^n$ such that $T(\underline{x}') = T(\underline{x})$.
		\item We denote the { set of all types} of all length-$n$ vectors over ${\cal X}$ as ${\cal P}_n(\cal X)$, and the set of all p.m.f.s over ${\cal X}$ as ${\cal P}(\cal X)$.
		\item The number of types of length-$n$ vectors over ${\cal X}$ satisfies $|{\cal P}_n({\cal X})| = {{n+|{\cal X}|-1}\choose{|{\cal X}|-1}} \leq (n+1)^{|{\cal X}|-1}$.
\end{enumerate}

\item {\bf \underline{Size of a type-class of a binary vector}}: Let $\underline{x} \in \{0,1\}^n$ have $qn$ ones and $n(1-q)$ zeroes. Then $|{\cal T}(\underline{x})| = {{n}\choose{qn}} \doteq 2^{nH(q)}$, where $H(q) \triangleq q\log \left (\frac{1}{q} \right)+ (1-q)\log \left (\frac{1}{1-q} \right) $.

\item {\bf \underline{Probability of subverting expectations for binary random variables}}: For binary random variables $X_i \sim p_X = (1-p,p)$, the probability that $T(\underline{x})$, the type of $\underline{x}$, equals some $q_X = (1-q,q)$ satisfies
\begin{equation*}
	\Pr[T(\underline{x}) = q_X] = 	{{n}\choose{qn}} p^{pn}(1-p)^{(1-p)n} \doteq 2^{-nD(q||p)},
\end{equation*}
\noindent where $D(q||p) \triangleq q\log \left (\frac{q}{p} \right)+ (1-q)\log \left (\frac{1-q}{1-p} \right) $.

\item {\bf \underline{(Binary) Pinsker's inequality}}: $D(q||p)\geq \frac{2}{\ln(2)}|p-q|^2$

\item {\bf \underline{Chernoff bound (additive form)}}: $\Pr[|\bar{X}  -{\mathbb E}\bar{X}| \geq 2\epsilon n] \dot{\leq} \exp \left (-\frac{2}{\ln(2)}n\epsilon^2 \right )$\\
Hence if $\epsilon \in \omega\left (\frac{1}{\sqrt{n}} \right)$, then $\Pr[|\bar{X}  -{\mathbb E}\bar{X}| \geq 2\epsilon n] \leq 1-o(1)$.

\item {\bf \underline{Entropy perturbation bound (for general alphabets)}}: Let $p_X$ and $q_X$ be two distributions on ${\cal X}$ such that $||p_X- q_X||_1 = \epsilon \leq \frac{1}{2}$. \\Then $|H(p_X) - H(q_X)| \leq \epsilon \log \left( \frac{|\cal X|}{\epsilon} \right )$

\item {\bf \underline{(Binary) high probability sets}}: Let ${\cal B} \subseteq \{0,1\}^n$ be the smallest set such that $\Pr({\underline{X}}\in {\cal B}) = 1-o(1)$. Then one can construct ${\cal B}$ greedily, and $|{\cal B}| \dot{\leq} 2^{nH(p) + n\epsilon\log(2/\epsilon)}$

\item {\bf \underline{Strongly typical sets (for general alphabets)}}: Given $p_X$ over alphabet ${\cal X}$, the {\it $(\epsilon,n)$-strongly typical set}, denoted ${\cal A}^{(n)}_{\epsilon}(X)$ is defined as $\{{\underline{x}} \in  {\cal X}^n : ||T(\underline{x}) - p_X||_1 \leq \epsilon\}$. By Sanov's Theorem (see Problem Set 1), strongly typical sets are also high probability sets.

\item {\bf \underline{Lexicographic encoding (binary vectors)}}: Lexicographic encoding allows one to compress a string $\underline{X}$ with components drawn i.i.d. according to $p_X$ into a string $\underline{Y}$ with expected length ${\mathbb E}(len(\underline{Y})) = nH(X) + o(n)$, such that $\underline{X}$ can be perfectly reconstructed from $\underline{Y}$, by first describing the type of $\underline{X}$, and then describing the index of $\underline{X}$ within ${\cal T}(\underline{X})$. Lexicographic encoding/decoding can be implemented in a recursive fashion, with overall computational complexity that is ${\cal O}(n^3\log^2(n))$.

\end{enumerate}



\end{document}

